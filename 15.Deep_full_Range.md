@[TOC](Deep-full-Range:deep-learning based network encrypted traffic classification and intrusion detection framework)

一种轻量级的，加密流量分类和入侵检测系统。

# Introduction

- 流量种类增多
- 使用安全协议HTTPS，SSH，SSL
- 因为隐私和法律，流量元数据难以获取，包括流的容量volume，流持续时间
- 过去的方法多是基于流分析+机器学习的方法，包括k近邻算法和决策树方法。
  - 需要人工干预，成本高，影响数据原始特征（裁剪导致流长度变化）
  - 机器学习计算和存储需求高，无法部署在本地节点（汽车、家庭网关、手机等等），例如sentinel就是把分类任务交给远端安全服务器，本地网关只负责执行。
- 基于深度学习的方法可以避免手动选取特征和获取隐私数据特征的问题，且学习能力更强，能学到更复杂的特征模式。
  - 能够在一个框架内实现NID，关键链接分析（critical link analyze），流量分类。
- 本系统使用
  - CNN，用于获取原始流量的空间特征
  - LSTM，用于获取特征的时间特征
  - SAE，从编码特性上提取特征
- 只保存在当前流量环境下的最好模型，节约存储

贡献

- 轻量级流量分类和入侵检测框架。
- 使用多个深度学习模型，获取流量的多维特征。
- 本模型是一个端到端模型，不需要过多的人工干预和隐私信息。

# Framework

数据预处理和DFR主体部分

![image-20231030185340534](C:\Users\8208191402\Desktop\笔记\图片\image-20231030185340534.png)

术语

![494F70D14223061CDAF62C41BEDB83BD](D:\QQ文件\MobileFile\494F70D14223061CDAF62C41BEDB83BD.png)

数据预处理：

- 原始数据长度不一，不适合神经网络的输入
- 原始数据的某些字段会影响推理结果，包括端口号和MAC地址
- 统一的输入格式对于后续其他网络应用有帮助
- ![image-20231030185842657](C:\Users\8208191402\Desktop\笔记\图片\image-20231030185842657.png)

- 数据包生成：把连续的原始流量，通过wireshark切分成PCAP文件。把一个流中的，网络所有层的双向数据包保存到一个文件。

- 流量净化：消除流量数据包中的干扰字段，包括TCP/UDP header和链路层Ethernet相关的数据（MAC地址）

  > MAC 地址和 TCP 标头可能会干扰流量分类，因为它们用于网络堆栈的不同层。 以下是一些原因：
  >
  > - MAC 地址用于数据链路层：MAC 地址用于识别本地网络上的设备，并在制造过程中硬编码到设备中。它们用在以太网帧头中以确保节点到节点的通信，但是，它们不用于数据包传输或数据包转发。因此，如果网络设备移动到不同的网络，其 MAC 地址将会发生变化，这可能会导致流量分类问题。
  > - TCP 标头用于传输层：TCP 标头用于建立和维护两个设备之间的连接并确保可靠的数据传输。它们包含源端口号和目标端口号、序列号和确认号等信息。但是，某些应用程序使用非标准端口号或以其他方式修改 TCP 标头，这使得仅根据 TCP 标头对流量进行分类变得困难。
  > - 网络堆栈的不同层：MAC 地址用于数据链路层，而 TCP 标头用于传输层。**流量分类通常在网络层完成，位于数据链路层和传输层之上**。因此，使用 MAC 地址或 TCP 标头对流量进行分类可能会出现问题，因为它们不是为此目的而设计的。
  > - 总之，MAC 地址和 TCP 标头可能会干扰流量分类，因为它们用于网络堆栈的不同层，并且不是为此目的而设计的。 为了准确地对流量进行分类，可以使用其他方法，例如深度数据包检查或机器学习。

- 流量细化，去除重复的文件和空文件。

- 统一长度，将文件长度统一成900字节，不足用0X00填充

- IDX文件生成，把固定长度的文件转换成30*30的二维IDX文件。这些文件映射到0-1之后可以做灰度图像。

- ![image-20231030190856992](C:\Users\8208191402\Desktop\笔记\图片\image-20231030190856992.png)

# Deep_full_Range

使用CNN，LSTM，SAM分别进行处理，使用L1正则化来惩罚权重参数。

## 1D CNN-BASED classfier

CNN适合以像素为单位学习一张图的空间属性，因此使用CNN来获取流量分类过程中的空间特征。因为原始流量是一个由包含等级关系的包序列组成的，因此使用一维卷积。

> 在处理等级形式组织的序列向量时，通常使用1维卷积（也称为一维卷积）比2维卷积效果更好，因为1维卷积更适合处理一维序列数据，能更好地捕捉序列中的局部模式和特征。
>
> 一维卷积是专门设计用于处理一维序列数据的卷积操作，例如时间序列数据、文本数据或音频数据等。它在一维方向上滑动卷积核以检测序列中的模式和特征，因此更适合处理等级形式组织的序列向量。
>
> 2维卷积通常用于处理图像数据，它在两个方向上滑动卷积核，用于捕捉图像中的空间特征。当应用到一维序列数据时，2维卷积可能会捕获不必要的多余信息，导致模型性能下降。
>
> 所以，如果你要处理等级形式组织的序列向量，建议使用1维卷积来提取序列中的特征。当然，最佳的方法还是进行实验和调整，以确定哪种卷积操作对于特定问题和数据集的效果更好。

![image-20231030191258683](C:\Users\8208191402\Desktop\笔记\图片\image-20231030191258683.png)

包含2个卷积与池化，2个Local Response Normalization层，以及一个使用softmax分类器的全连接层。处理时，

- 把输入转化成1*900的向量，

- 第一个卷积层使用32个25\*1的卷积核，卷积步长为1。

- 使用ReLU激活函数，

- 池化层对每个输入的[3,1]维向量，取局部最大值，池化步长为3。
  $$
  maxpolling[x1,x2,x3] =max(x1,x2,x3)
  $$

  > 当最大池化（MaxPooling）的步长（stride）和窗口大小（kernel size）相同时，没有重叠的窗口。这意味着每个窗口都覆盖了互补的部分，且窗口之间没有交叠，因此输出的维度会相应地减小。
  >
  > 效果包括以下几点：
  >
  > 1. 减小特征图的尺寸：由于窗口之间没有重叠，每个窗口的最大值都会对应一个输出位置，因此输出的尺寸会减小。
  > 2. 强调局部特征：因为每个窗口内只选择最大值，MaxPooling有助于强调局部特征，提取出最显著的特征，同时减小了对细节的敏感性。这可以帮助模型更好地捕获重要特征。
  > 3. 位置信息丧失：与较小的步长和/或窗口重叠的MaxPooling相比，步长和窗口大小相同的MaxPooling会丧失一些位置信息，因为输出的位置没有考虑输入中的位置之间的关系。这可能会对一些任务（如目标定位）造成一定影响。
  >
  > 总之，使用相同的步长和窗口大小的MaxPooling可以减小特征图的尺寸，强调局部特征，提高计算效率，但也可能导致位置信息的丧失。

- 使用LRN来惩罚异常响应（response）或异常输出层，来获得更好的泛化性。

  > 一些常见的异常响应模式可能包括：
  >
  > 1. **信息丢失**：某些情况下，池化操作可能导致信息的不可逆丢失。例如，在最大池化中，选择每个窗口内的最大值可能会导致一些次要特征被忽略，从而丧失了信息。
  > 2. **不合理的响应**：某些情况下，池化操作可能在某些特征上产生不合理的响应。例如，当输入包含异常值或噪音时，池化层的响应可能不稳定或异常。
  > 3. **不适当的窗口大小**：选择池化窗口的大小可能影响到响应。如果窗口过小，可能会丧失大范围的全局信息，如果窗口过大，可能会导致分辨率降低。不适当的窗口大小可能导致异常响应。

- 进入第二个卷积层，使用64个核，后续相同。

- 全连接层输出多个结果，使用softmax分类器输出概率密度
  $$
  \hat{y}=\frac{exp(Out^j)}{\sum exp(Out^i)}\\
  exp(Out^j)是第j个神经元输出\\
  选概率最高的作为预测分类\hat{y}
  $$

使用Adam优化器，来计算

- 每个参数各自的学习率
- 参数的一阶矩估计、二阶矩估计、矩估计的衰减率（当样本数太少不满足大数定理时，矩估计会出现衰减）等等

本训练过程的超参数(见table1)：

```json
{训练周期Epoch,batch大小:Minbatch,学习率：LR,dropout保留率：KeepP,类的总数：N,L1正则化的参数：Lambda}
```

![image-20231030192702839](C:\Users\8208191402\Desktop\笔记\图片\image-20231030192702839.png)

## LSTM-BASED classfier

LSTM是一种RNN，用于获取时序信息的网络。因为一个traffic segment是一系列按顺序生成的包和字节流，因此同一类流量具有时序上的相似性特征。在本系统中，LSTM的输入是一个灰度图。

![image-20231030193036385](C:\Users\8208191402\Desktop\笔记\图片\image-20231030193036385.png)

LSTM分类器由三层LSTM组成，每层256个LSTM cell，每层使用dropout来获取泛化特征。获取的特征同样经过softmax分类器，使用和CNN相同的分类方式。分类结果在最后一个模块检测性能（见图1）。

同样使用Adam优化器，使用和CNN相同的参数，即形如

```xml
{Epoch,Minbatch,LR,KeepP,N,Lambda}
```

![image-20231030193619649](C:\Users\8208191402\Desktop\笔记\图片\image-20231030193619649.png)

## SAT-BASED classfier

> Autoencoder（自编码器）是一种无监督学习的神经网络模型，用于学习数据的压缩表示。它的主要目标是将输入数据进行编码（压缩）成一个低维度的表示，然后再将该表示解码（恢复）回原始数据，使重建的数据尽可能接近输入数据。
>
> Autoencoder的结构通常包含三个主要部分：
>
> 1. **编码器（Encoder）**：编码器将输入数据映射到低维度的表示，通常是一个编码向量或编码矩阵。编码器的任务是捕捉输入数据中的关键特征，并将其表示为一个紧凑的形式。
> 2. **解码器（Decoder）**：解码器将编码后的表示映射回原始数据的空间。解码器的任务是将低维度表示转换为与原始输入尺寸相同的重建数据。
> 3. **损失函数（Loss Function）**：损失函数用于衡量重建数据与原始输入数据之间的差异。Autoencoder的目标是最小化这个差异，从而使重建数据尽可能接近原始数据。
>
> Autoencoder的训练过程包括将输入数据通过编码器进行编码，然后通过解码器进行解码，计算重建数据与原始数据之间的差异，然后反向传播误差，更新模型的权重和偏置，以减小损失函数。在训练完成后，编码器部分的参数可以用来获取数据的压缩表示。
>
> Autoencoder在多个领域有广泛的应用，包括特征学习、降维、去噪、图像压缩、生成模型等。它可以用来自动学习数据的有用表示，从而可以在许多机器学习任务中提供有益的特征提取。

由两个SAE组成。自编码器是一种半监督的，自动提取特征的学习方法。SAE作为AE的变种，可以通过以字节为单位扫描数据，获取编码形式的特征。

![image-20231030193905318](C:\Users\8208191402\Desktop\笔记\图片\image-20231030193905318.png)

- 把输入图转换成1*900的向量
- 独立训练两个SAE，
- 第一个SAE有1000个神经元，与900个输入和900个输出全连接，目的是得到一种具有最小损失的[900->900]编码方式
- 把第一个SAE压栈，使用sigmold激活函数
- 第一个SAE有1500个神经元，与900个输入和900个输出全连接，具体和SAE1类似。
- 把第二个SAE压栈，使用sigmold激活函数
- 执行fine-tune训练过程，得到最终模型

因为两个SAE是单独训练的，超参数选择和之前不同。形如

```xml
Epoch(两个SAE都训练一次的次数，不能太大)，EpochFin(fine-tune训练次数，无限制)，Lambda(两个SAE训练时的L1正则化参数，要比后一个lambda大很多，否则就不叫fine-tune了)，LambdFin；setsame={Epoch,Minbatch,LR,KeepP,N,Lambda}
```

包含额外参数和与之前模块的共享参数

![image-20231030194741188](C:\Users\8208191402\Desktop\笔记\图片\image-20231030194741188.png)

## select&save

对上述DFR模块的训练结果进行测试。最高准确度的模型被选为最适合目前流量环境的模型。
$$
Accuracy=\frac{TP+TN}{TP+TN+FP+FN}
$$
选中的模块在这个时间单元上执行分类和设备识别任务。DFR使用在线训练，因此能自动根据流量环境更新。

![image-20231030195138732](C:\Users\8208191402\Desktop\笔记\图片\image-20231030195138732.png)

# Evaluation

数据集为两个独立数据集，

- 一个为ISCX VPN-nonVPN trafc dataset，用于测试加密流量分类任务，包含7种常规加密流量和7种隐藏协议的加密流量，本实验使用前7种：包含网页浏览、邮件、聊天、流媒体、文件传输、VoIP、P2P。因为后6种流量都和网页浏览相关，所以舍弃第一个class，丢弃不平衡类的样本数量来平衡样本。
- 一个是ISCX 2012 IDS dataset，用于测试入侵检测任务，包含5类流量：Normal、Brute Force SSH、DDoS、HttpDoS、Infiltrating。因为日常流量中，恶意流量数量少于normal流量数量，因此不对样本进行数量正则化。
- ![image-20231030195921811](C:\Users\8208191402\Desktop\笔记\图片\image-20231030195921811.png)

环境

i7-7700K，1080Ti，32G memory

![image-20231030195933868](C:\Users\8208191402\Desktop\笔记\图片\image-20231030195933868.png)

## 测试结果

### 加密流量分类任务

![image-20231030200030291](C:\Users\8208191402\Desktop\笔记\图片\image-20231030200030291.png)

使用L1正则化代替L2之后，准确率提高约3%。1D CNN准确率最高，选为当前最优模型，用于当前时间元的设备识别和分类任务。

![image-20231030200040521](C:\Users\8208191402\Desktop\笔记\图片\image-20231030200040521.png)

把我们的工作和另两个工作对比：C4.5 Decision Tree和没有使用正则化以及LRN层的CNN。

### 入侵检测效果

和目前的机器学习算法比较，分别是DT和KNN。

![image-20231030200415046](C:\Users\8208191402\Desktop\笔记\图片\image-20231030200415046.png)

在HttpDoS上效果提升显著。

### 存储空间需求比较

- CNN：1534KB
- LSTM：186KB
- SAE：12732KB
- KNN：67548KB
- DT：34247KB

